Loading pipeline components...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36.98it/s]
Traceback (most recent call last):
  File "/home/sehwan/MIIL/diffusion-distillation/memo.py", line 391, in <module>
    app.run(main)
  File "/home/sehwan/anaconda3/envs/SLD/lib/python3.9/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/sehwan/anaconda3/envs/SLD/lib/python3.9/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/sehwan/MIIL/diffusion-distillation/memo.py", line 387, in main
    distill_caching_random2()
  File "/home/sehwan/MIIL/diffusion-distillation/memo.py", line 338, in distill_caching_random2
    teacher_model = UNetDiffusers.from_pretrained(pretrained_model_id).to(device)
  File "/home/sehwan/anaconda3/envs/SLD/lib/python3.9/site-packages/diffusers/models/modeling_utils.py", line 621, in from_pretrained
    raise ValueError(
ValueError: Cannot load <class 'model.UNetDiffusers'> from google/ddpm-cifar10-32 because the following keys are missing:
 down_blocks.2.attentions.1.group_norm.weight, up_blocks.0.attentions.1.to_v.weight, up_blocks.0.attentions.2.to_v.weight, up_blocks.1.attentions.1.to_out.0.weight, down_blocks.3.attentions.0.group_norm.weight, down_blocks.2.attentions.1.to_v.weight, up_blocks.1.attentions.0.to_q.weight, down_blocks.2.attentions.0.group_norm.bias, down_blocks.2.attentions.0.to_k.weight, up_blocks.1.attentions.1.to_k.bias, up_blocks.0.attentions.1.to_out.0.bias, down_blocks.2.attentions.0.to_v.bias, up_blocks.0.attentions.1.to_k.bias, down_blocks.3.attentions.0.to_v.weight, down_blocks.2.attentions.1.to_out.0.bias, down_blocks.3.attentions.1.to_q.weight, up_blocks.1.attentions.2.to_v.weight, down_blocks.3.attentions.1.to_out.0.weight, up_blocks.0.attentions.1.group_norm.bias, up_blocks.1.attentions.1.to_v.bias, up_blocks.1.attentions.2.to_k.weight, down_blocks.2.attentions.1.group_norm.bias, up_blocks.0.attentions.0.to_q.bias, up_blocks.1.attentions.2.to_q.weight, down_blocks.2.attentions.1.to_k.bias, up_blocks.0.attentions.2.to_out.0.bias, up_blocks.0.attentions.2.to_k.bias, down_blocks.2.attentions.1.to_out.0.weight, down_blocks.3.attentions.1.to_v.bias, down_blocks.2.attentions.0.to_k.bias, up_blocks.1.attentions.0.to_k.bias, up_blocks.1.attentions.0.to_q.bias, down_blocks.2.attentions.0.group_norm.weight, down_blocks.3.attentions.1.to_out.0.bias, down_blocks.2.attentions.0.to_q.bias, down_blocks.3.attentions.0.to_out.0.bias, down_blocks.3.attentions.0.to_q.bias, down_blocks.3.attentions.0.to_k.weight, down_blocks.3.attentions.0.to_out.0.weight, up_blocks.1.attentions.0.to_out.0.bias, up_blocks.0.attentions.1.to_q.bias, up_blocks.0.attentions.2.to_out.0.weight, up_blocks.1.attentions.0.to_v.weight, up_blocks.1.attentions.2.to_v.bias, up_blocks.1.attentions.0.to_k.weight, up_blocks.0.attentions.2.to_q.weight, up_blocks.0.attentions.1.to_v.bias, down_blocks.2.attentions.0.to_q.weight, down_blocks.3.resnets.0.conv_shortcut.weight, up_blocks.0.attentions.1.to_k.weight, up_blocks.1.attentions.0.to_out.0.weight, up_blocks.0.attentions.2.group_norm.bias, up_blocks.0.attentions.0.to_v.bias, up_blocks.0.attentions.1.to_q.weight, down_blocks.2.resnets.0.conv_shortcut.weight, up_blocks.1.attentions.1.to_q.bias, up_blocks.0.attentions.0.to_k.bias, up_blocks.1.attentions.2.group_norm.bias, up_blocks.0.attentions.0.to_q.weight, down_blocks.2.attentions.0.to_out.0.bias, down_blocks.3.attentions.0.to_k.bias, up_blocks.1.attentions.1.to_out.0.bias, up_blocks.0.attentions.0.to_out.0.bias, up_blocks.0.attentions.0.group_norm.bias, up_blocks.1.attentions.1.group_norm.bias, down_blocks.2.attentions.0.to_v.weight, up_blocks.1.attentions.2.group_norm.weight, up_blocks.0.attentions.2.to_k.weight, down_blocks.3.attentions.1.group_norm.weight, up_blocks.1.attentions.2.to_out.0.weight, down_blocks.3.resnets.0.conv_shortcut.bias, down_blocks.3.attentions.1.to_v.weight, up_blocks.0.attentions.0.to_out.0.weight, up_blocks.1.attentions.2.to_out.0.bias, down_blocks.2.resnets.0.conv_shortcut.bias, up_blocks.0.attentions.2.to_v.bias, up_blocks.0.attentions.2.to_q.bias, down_blocks.3.attentions.1.to_k.weight, up_blocks.1.attentions.0.to_v.bias, down_blocks.3.attentions.1.to_k.bias, up_blocks.1.attentions.2.to_k.bias, down_blocks.2.attentions.1.to_q.bias, down_blocks.3.attentions.1.group_norm.bias, up_blocks.0.attentions.2.group_norm.weight, down_blocks.2.attentions.1.to_k.weight, up_blocks.1.attentions.0.group_norm.bias, up_blocks.0.attentions.0.to_k.weight, up_blocks.0.attentions.0.group_norm.weight, up_blocks.1.attentions.2.to_q.bias, down_blocks.2.attentions.1.to_q.weight, up_blocks.1.attentions.1.to_v.weight, up_blocks.0.attentions.1.group_norm.weight, up_blocks.0.attentions.1.to_out.0.weight, down_blocks.3.attentions.0.to_v.bias, down_blocks.3.attentions.0.to_q.weight, up_blocks.1.attentions.1.to_k.weight, up_blocks.1.attentions.0.group_norm.weight, down_blocks.2.attentions.0.to_out.0.weight, up_blocks.1.attentions.1.group_norm.weight, down_blocks.3.attentions.1.to_q.bias, up_blocks.0.attentions.0.to_v.weight, down_blocks.2.attentions.1.to_v.bias, up_blocks.1.attentions.1.to_q.weight, down_blocks.3.attentions.0.group_norm.bias.
 Please make sure to pass `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize those weights or else make sure your checkpoint file is correct.